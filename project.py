# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nX0WGHXGRYfBepaMHmuAQFGkUqa03HON

---

**GENRE**

"""


import time

pip install IMDbPY
import imdb

def genre(gen):
  switcher={
            'Adventure':1,
            'Action':2,
            'Drama':4,
            'Comedy':3,
            'Thriller':5,
            'Horror':7,
            'Crime':6,
            'Western':10, 
            'Short':11,
            'Animation':8,
            'History':9,
             }
  return switcher.get(gen,6)

def Imdb(movie):
  try:
    o = imdb.IMDb()
    a = o.search_movie(movie)
    b = o.get_movie(a[0].movieID)
    c = b.get('genres')
    return(genre(c[0]))
    
  except:
    return(0)


"""
**GOOGLE_NEWS**
"""



pip install GoogleNews
from GoogleNews import GoogleNews
googlenews = GoogleNews()
pip install vadersentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def vader(text):
  analyzer = SentimentIntensityAnalyzer()
  sent = analyzer.polarity_scores(text)
  return sent

def average(arr):
  a = 0
  for i in range(len(arr)):
    a = a + arr[i]['compound']
  try: 
    a = (a/len(arr))
    a = round(a,1)
    return(a)
  except ZeroDivisionError:
    return(0)

def google(key):
  googlenews.clear()
  c = googlenews.search(key)
  a = googlenews.gettext()
  
  arr = []
  for i in range(len(a)):
    b = vader(a[i])
    arr.append(b)
    
  num = average(arr)
  num = (num+1)/2
  
  return(num)

"""---

**TWITTER**

---
"""

import tweepy
import csv #Import csv
import os
#from google.colab import files
from textblob import TextBlob

# Consumer keys and access tokens, used for OAuth
consumer_key = '*******************************'
consumer_secret = '*******************************'
access_token = '*******************************'
access_token_secret = '*******************************'
# OAuth process, using the keys and tokens
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

def vader(text):
  analyzer = SentimentIntensityAnalyzer()
  sent = analyzer.polarity_scores(text)
  return sent

def pol(text):
  t = TextBlob(text)
  ti = t.sentiment.polarity
  return ti

def sub(text):
  t = TextBlob(text)
  ti = t.sentiment.subjectivity
  return ti

def tweet(movie):
  key = movie
  api = tweepy.API(auth,wait_on_rate_limit = True)
  #print(key)

  list_tweets = []
  #since=date_since
  #date_since = "2018-11-16"
  for status in tweepy.Cursor(api.search, q = key+"-filter:retweets", lang = 'en', result_type = 'mixed').items(100):
    list_tweets.append(status.text)
    
    
  
  import re
  list_new = []

  for tweet in list_tweets:
      tweet = re.sub(r"^https://t.co/[a-zA-Z0-9]*\s", " ", tweet)
      tweet = re.sub(r"\s+https://t.co/[a-zA-Z0-9]*\s", " ", tweet)
      tweet = re.sub(r"\s+https://t.co/[a-zA-Z0-9]*$", " ", tweet)
      tweet = tweet.lower()
      tweet = re.sub(r"that's","that is",tweet)
      tweet = re.sub(r"there's","there is",tweet)
      tweet = re.sub(r"what's","what is",tweet)
      tweet = re.sub(r"where's","where is",tweet)
      tweet = re.sub(r"it's","it is",tweet)
      tweet = re.sub(r"who's","who is",tweet)
      tweet = re.sub(r"i'm","i am",tweet)
      tweet = re.sub(r"she's","she is",tweet)
      tweet = re.sub(r"he's","he is",tweet)
      tweet = re.sub(r"they're","they are",tweet)
      tweet = re.sub(r"who're","who are",tweet)
      tweet = re.sub(r"ain't","am not",tweet)
      tweet = re.sub(r"wouldn't","would not",tweet)
      tweet = re.sub(r"shouldn't","should not",tweet)
      tweet = re.sub(r"can't","can not",tweet)
      tweet = re.sub(r"couldn't","could not",tweet)
      tweet = re.sub(r"won't","will not",tweet)
      tweet = re.sub(r"\W"," ",tweet)
      tweet = re.sub(r"\d"," ",tweet)
      tweet = re.sub(r"\s+[a-z]\s+"," ",tweet)
      tweet = re.sub(r"\s+[a-z]$"," ",tweet)
      tweet = re.sub(r"^[a-z]\s+"," ",tweet)
      tweet = re.sub(r"\s+"," ",tweet)

      #print(tweet,'\n')
      list_new.append(tweet)
  #return(list_new)
      
  
  def vader_1(list_new):
    arr = []
    for i in range(len(list_new)):
      v = vader(list_new[i])
      arr.append(v)


    a = 0
    for i in range(len(arr)):
      a = a + arr[i]['compound']
    try: 
      a = (a/len(arr))#*33.3
      a = round(a,2)
      return(a)
    except ZeroDivisionError:
      return(0)
    
    
    
  
  def polar_1(list_new):
    arr = []
    for i in range(len(list_new)):
      v = pol(list_new[i])
      arr.append(v)


    a = 0
    for i in range(len(arr)):
      a = a + arr[i]
    try: 
      a = (a/len(arr))
      a = round(a,2)
      return(a)
    except ZeroDivisionError:
      return(0)
    
    
    
    
  def subject_1(list_new):
    arr = []
    for i in range(len(list_new)):
      v = sub(list_new[i])
      arr.append(v)


    a = 0
    for i in range(len(arr)):
      a = a + arr[i]
    try: 
      a = (a/len(arr))
      a = round(a,2)
      return(a)
    except ZeroDivisionError:
      return(0)
  
  v = vader_1(list_new)
  p = polar_1(list_new)
  s = subject_1(list_new)
  
  result = [v,p,s]
  return(result)

"""---

**MODEL**

---
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor

from sklearn.model_selection import cross_val_score as cvs
from sklearn.model_selection import train_test_split
from sklearn import neighbors, datasets, preprocessing
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error as mse

df = pd.read_csv("/home/dhairya/Desktop/FINAL_DATASET.csv", header = 0)

'''df = df.drop("Unnamed: 0", axis = 1)'''
#df = df.drop('Vader', axis = 1)
#df = df.drop('Polar', axis = 1)
#df = df.drop('Subject', axis = 1)
#df = df.drop('Genre', axis = 1)
#df = df.drop('G_news', axis = 1)
df = df.set_index("Title")
df1 = df.values
X = df1[:,0:5]
Y = df1[:,5]

df.count()

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

scaler = preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

#reg = DecisionTreeRegressor(random_state = 0)
#reg.fit(X_train,y_train)


#reg= RandomForestRegressor(n_estimators = 300, random_state = 0)
#reg.fit(X_train, y_train)


#reg = SVR(kernel='rbf')
#reg.fit(X_train,y_train)


#reg = MLPRegressor(solver='sgd', max_iter=100, activation='relu',random_state=1, learning_rate_init=0.01,batch_size=X.shape[0])
#reg.fit(X_train, y_train)


reg = LinearRegression()  
reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)
y_pred = np.round(y_pred, 1)
mae(y_test, y_pred)

mse(y_test, y_pred)

import numpy as np

def mape(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape(y_test, y_pred)


"""
**RUN**
"""
start = time.time()
def project(movie):
  g = Imdb(movie)
  gn = google(movie)
  t = tweet(movie)
  return([g, gn, t[0], t[1], t[2]])

movie = 'Dil Bechara'

film = project(movie)

film

output = reg.predict([film])
output = round(output[0],1)

print("time taken:",time.time()-start)
print("rating of movie {0} is {1}".format(movie, output))



